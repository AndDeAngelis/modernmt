#!/usr/bin/env python3
import argparse
import os
import random
import sys

__description = '''
Human Evaluation Tool is a script to manually evaluate the translations
generated by 'evaluate' command with the option '--human-eval'.

List of commands:
    - 0..9:     vote the option that produced the best translation
    - b:        if both translations are good
    - n:        if none of the translation options are acceptable
    - ctrl+c:   interrupt the evaluation and print results
'''


class Translation:
    def __init__(self, _id):
        self.id = _id
        self.source = None
        self.reference = None
        self.mts = {}


class Translations:
    @staticmethod
    def __load(filename, output, key):
        with open(filename) as stream:
            for line in stream:
                _id, lang, line = line.split('\t', 2)
                line = line.strip()

                if _id in output:
                    translation = output[_id]
                else:
                    output[_id] = translation = Translation(_id)

                if key == '#ref':
                    translation.reference = line
                elif key == '#src':
                    translation.source = line
                else:
                    translation.mts[key] = line

    def __init__(self, path, random_seed=None, whitelist=None):
        source = None
        reference = None
        mts = []

        for f in os.listdir(path):
            filename = f[:f.rfind('.')]
            filepath = os.path.join(path, f)

            if 'source' == filename:
                source = filepath
            elif 'reference' == filename:
                reference = filepath
            elif len(filename) > 0 and (whitelist is None or filename in whitelist):
                mts.append(filepath)

        if reference is None:
            raise Exception('Missing reference file')
        if source is None:
            raise Exception('Missing source file')
        if len(mts) < 2:
            raise Exception('Not enough machine translated files: ' + ', '.join(mts))

        self.mts = []
        self._translations = {}
        self.__load(reference, self._translations, '#ref')
        self.__load(source, self._translations, '#src')
        self._random_seed = random_seed

        for mt in mts:
            name = os.path.basename(mt)
            name = name[:name.rfind('.')]
            self.__load(mt, self._translations, name)
            self.mts.append(name)

    def __iter__(self):
        values = list(self._translations.values())

        if self._random_seed:
            random.Random(self._random_seed).shuffle(values)

        for translation in values:
            yield translation

    def __len__(self):
        return len(self._translations)


class ScoreBoard:
    def __init__(self, mts):
        self.mts = mts
        self.count = 0
        self.scores = {}
        self.both_good = 0
        self.both_bad = 0
        self.same_output = 0

        for mt in mts:
            self.scores[mt] = 0

    def set_both_good(self):
        self.both_good += 1
        self.count += 1

    def set_both_bad(self):
        self.both_bad += 1
        self.count += 1

    def set_wins(self, winner):
        self.scores[winner] += 1
        self.count += 1

    def set_same_output(self):
        self.same_output += 1

    def __str__(self):
        string = []

        def key2desc(_key):
            return _key.replace('_', ' ') + ' is best'

        ll = 9
        for key in self.mts:
            ll = max(ll, len(key2desc(key)))

        total = self.count + self.same_output

        for mt, victories in self.scores.items():
            string.append(
                key2desc(mt).ljust(ll) + (':  %d/%d (%.1f%%)' % (victories, total, victories * 100 / total)))

        string.append(
            'Both good'.ljust(ll) + (
                        ':  %d/%d (%.1f%%)' % (self.both_good, total, self.both_good * 100. / total)))
        string.append(
            'Both bad'.ljust(ll) +
            (':  %d/%d (%.1f%%)' % (self.both_bad, total, self.both_bad * 100. / total)))

        if self.same_output > 0:
            string.append(
                'Same'.ljust(ll) +
                (':  %d/%d (%.1f%%)' % (self.same_output, total, self.same_output * 100. / total)))

        return '\n'.join(string)

    def __repr__(self):
        return self.__str__()


def evaluate(translation, scoreboard, skip_equals=False):
    mts = scoreboard.mts[:]
    random.shuffle(mts)

    if skip_equals and len(set(translation.mts.values())) == 1:
        scoreboard.set_same_output()
        return False

    print('LINE %d' % (scoreboard.count + 1))
    print('SOURCE:'.rjust(12), translation.source)
    print('REFERENCE:'.rjust(12), translation.reference)
    for i in range(0, len(mts)):
        mt = mts[i]
        line = translation.mts[mt]
        print(('OPTION %d:' % i).rjust(12), line)
    print()

    while True:
        print('0..9, [B]oth or [N]one >> ', end='', flush=True)
        read_input = sys.stdin.readline().strip().lower()

        try:
            i = int(read_input)
            if 0 <= i < len(mts):
                scoreboard.set_wins(mts[i])
                break
        except ValueError:
            if read_input == 'b':
                scoreboard.set_both_good()
                break
            elif read_input == 'n':
                scoreboard.set_both_bad()
                break
            elif read_input == 'x':
                return True
    print()
    return False


def main():
    parser = argparse.ArgumentParser(description='Simple script to compare translations generated by Evaluate command.')

    parser.add_argument('path', metavar='PATH', help='the path to the Human Evaluation folder '
                                                     'created by Evaluate command.')
    parser.add_argument('-r', '--random-seed', dest='seed', type=int, default=None,
                        help='a random seed used to shuffle sentences (by default no shuffling is performed)')
    parser.add_argument('--mts', dest='mts_list', nargs='+', default=None,
                        help='explicitly list the MTs you want to compare (ignore the rest)')
    parser.add_argument('--skip-equals', dest='skip_equals', action='store_true', default=False,
                        help='skip lines that have the same translation')
    args = parser.parse_args()

    translations = Translations(args.path, random_seed=args.seed, whitelist=args.mts_list)
    scores = ScoreBoard(translations.mts)

    os.system('clear')
    print(__description)
    print('Number of lines in test set: %d' % len(translations), end='\n\n')

    try:
        for translation in translations:
            if evaluate(translation, scores, skip_equals=args.skip_equals):
                break
    except KeyboardInterrupt:
        pass
    except Exception as e:
        print('ERROR: %s\n' % str(e), end='\n\n', file=sys.stderr)
        return 1

    if scores.count > 0:
        print('\n============== RESULTS ==============\n')
        print(scores)
        print()

    return 0


if __name__ == '__main__':
    exit(main())
